{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMPIRICAL MACROECONOMICS\n",
    "*Francesco Franco, Nova SBE*\n",
    "## Class 1 Python warm-up on Sims lecture\n",
    "Words in bold are commands to be typed in the Anaconda prompt.\n",
    "### Setting up the class environment\n",
    "\n",
    "1. Install [Anaconda](https://www.anaconda.com/). \n",
    "   - To check the conda version: **conda list anaconda$**\n",
    "2. Create you work environment to be able to reproduce your work\n",
    "   - to check the environments you have: **conda env list**\n",
    "   - to create the Class environment: **conda create -n ClassEM python=3.6 notebook numpy pandas matplotlib xlrd statsmodels pandas-datareader seaborn**\n",
    "   - to activate the environment: **conda activate ClassEM**\n",
    "   - to deactivate the environment: **conda deactivate**\n",
    "   - to list packages in your environment: **conda list -n ClassEM**\n",
    "   - to export your packages in a list: **conda list --export > package-list.txt**\n",
    "   - to create an environment with your packages: **conda create -n myenv --file package-list.txt**   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We always start the code by importing the required package into memory\n",
    "''' \n",
    "import numpy as np                                       #import package for data array manipulation\n",
    "import pandas as pd                                      #import package for data analysis         \n",
    "from scipy.optimize import minimize                      #import package for scientific computing\n",
    "import matplotlib.pyplot as plt                          #import package for plotting\n",
    "from pandas.util.testing import assert_frame_equal\n",
    "from pandas_datareader.data import DataReader            # DataReader allows to download data from the main online\n",
    "                                                         # repositories : Eurostat, FRED, OECD, ..\n",
    "import seaborn as sns\n",
    "from scipy.stats import multivariate_normal,norm, invgamma, uniform "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package Pandas: forget EXCEL\n",
    "Help for common packages is accessible through the help tab of the Jupyter notebook\n",
    "\n",
    "This section loads the data and present basic data manipulation commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading and exploring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PANDAS_DATAREADER, simplest tool. we will see more advanced ways to download data using SDMX\n",
    "'''\n",
    "start = '1929' #start date                                             \n",
    "end = '1943' #end date\n",
    "data = DataReader(['PCECCA','GPDICA','GCECA'], 'fred', start=start, end=end) #Data from FRED, you need to find the code in the webiste\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info() # the info method gives you a lot of information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head() # check the first 5 rows with method .head(). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create output\n",
    "$$Y_t = C_t + I_t + G_t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haavelmo_data = data.copy() # create a new copy\n",
    "haavelmo_data.columns = ['c', 'i', 'g'] # rename columns\n",
    "haavelmo_data['y'] = haavelmo_data['c'] + haavelmo_data['g'] + haavelmo_data['i'] # create a new column with output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "of course you can read from files, csv are prefererd but almost any format is readable\n",
    "you need xlrd module for excel files \n",
    "'''\n",
    "df = pd.read_excel('../Data/Sims.xls')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() # Check the data with the method .info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T # Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head() # check the first 5 rows with method .head(). NOTICE indexing starts with 0 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Selecting a column or a row : there are many ways to select a comun let us converge on one\n",
    "let us reduce the data set to the first 5 rows\n",
    "'''\n",
    "df5 = df.head().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5['PCECCA'] # select a column with the name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Purely integer-location based indexing for selection by position .iloc \n",
    "nice but you need to know the location\n",
    "'''\n",
    "df5.iloc[1:3,0:4] # notice it starts from index 1 (row 2) up to index 2 but does not include index 3 same for columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Purely label-location based indexer for selection by label .loc\n",
    "can build very complex queries in your table\n",
    "'''\n",
    "sel = (df5['date']>='1930-01-01') & (df5['date']<='1932-01-01') # logical operator &\n",
    "df5.loc[sel,['date','GPDICA','Y']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operators\n",
    "we have used a logical operator in the above selection\n",
    "- $|$ :or\n",
    "- & :and\n",
    "- $>$ :gt\n",
    "- $<$ :lt\n",
    "- $>=$ :ge\n",
    "- $<=$ :le\n",
    "- $==$ :eq\n",
    "- $!=$ :ne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns # print the name of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'Y': 'GDP'},inplace=True) # rename a column, attention to inplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns # print the name of the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Built-in function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['GDP'].sum() # as a method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df['GDP']) # as a function (procedure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 100 df['GDP'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 100 sum(df['GDP'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package Numpy: numerical operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "if speed is really an issue, namely BIG dataset, or loops, then transform dataframe into numpy arrays\n",
    "'''\n",
    "GDP = np.array(df['GDP'])\n",
    "%timeit -n 100 np.sum(GDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package Matplotlib: plot everything\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "plot Haavelmo data\n",
    "'''\n",
    "plt.figure(figsize=(12,6)) # declare a figure\n",
    "plt.plot(haavelmo_data) # plot the data\n",
    "plt.xlabel('Date') # add labels and title\n",
    "plt.ylabel('Billion Chained Dollar')\n",
    "plt.title('Haavelmo data')\n",
    "plt.legend(haavelmo_data.columns) # add legend\n",
    "plt.grid() # add a grid\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haavelmo_data.plot(grid=True,figsize=(12,6)) # Note that Panda as a plot method, slighlty less control but very practical\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Subplots\n",
    "'''\n",
    "plt.figure(figsize=(16,10))\n",
    "plt.subplot(221)\n",
    "plt.plot(haavelmo_data.c)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Billion Chained Dollar')\n",
    "plt.title('Consumption')\n",
    "plt.grid()\n",
    "plt.subplot(222)\n",
    "plt.plot(haavelmo_data.i)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Billion Chained Dollar')\n",
    "plt.title('Investment')\n",
    "plt.grid()\n",
    "plt.subplot(223)\n",
    "plt.plot(haavelmo_data.g)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Billion Chained Dollar')\n",
    "plt.title('Government expenditure')\n",
    "plt.grid()\n",
    "plt.subplot(224)\n",
    "plt.plot(haavelmo_data.y)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Billion Chained Dollar')\n",
    "plt.title('Output')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haavelmo_data.plot(subplots=True,layout=(2,2),figsize=(16,10),grid=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Haavelmo model\n",
    "\n",
    "> Haavelmo Statistical Testing of Business-Cycle Theories\n",
    "The modified Haavelmo Model (Sims Nobel Lecture) is:\n",
    "\n",
    "\n",
    "$$C_t = \\beta + {\\alpha}Y_t+\\epsilon_t$$\n",
    "\n",
    "$$I_t = \\theta_0 + \\theta_1(C_t - C_{t-1}) + \\eta_t$$\n",
    "\n",
    "$$Y_t = C_t + I_t + G_t$$\n",
    "\n",
    "$$G_t = \\gamma_0 + \\gamma_1G_{t-1} + v_t$$\n",
    "\n",
    "Where $\\epsilon_t$,$\\eta_t$, and $v_t$ are Normally distributed disturbance with constant variance and zero mean.  Substituting for $Y_t$ the system is therefore\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "C_{t}\\left(1-\\alpha\\right)-{\\alpha}I_t -{\\alpha}G_t = \\beta + \\epsilon_t\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "-\\theta C_{t}+I_{t}=\\theta_0 -\\theta C_{t-1}+\\eta_{t}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "G_t = \\gamma_0 + \\gamma_1G_{t-1} + v_t\n",
    "\\end{equation}\n",
    "\n",
    "or in matrix terms\n",
    "\n",
    "$${\\Gamma_0}X_t = C + {\\Gamma_1}X_{t-1} + U_t$$\n",
    "where\n",
    "$$\\Gamma_0=\\begin{pmatrix}\n",
    "1-\\alpha&-\\alpha&-\\alpha\\\\\n",
    "-\\theta&1&0\\\\\n",
    "0&0&1\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "$$C=\\begin{pmatrix}\n",
    "\\beta\\\\\n",
    "\\theta_0\\\\\n",
    "\\gamma_0\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "\n",
    "The reduced form is\n",
    "$$X_{t}=\\Gamma_{0}^{-1}C+\\Gamma_{0}^{-1}\\Gamma_{1}X_{t-1}+\\Gamma_{0}^{-1}U_{t}$$\n",
    "\n",
    "$$X_{t}=A+BX_{t-1}+V_{t}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for the Model\n",
    "- Python is an “object-oriented programming language.” This means that part of the code is implemented using a special construct called classes.\n",
    "- Python also allow a \"a functional approach\" which is the one we will follow in the course.\n",
    "- Programmers use classes to keep related things together. This is done using the keyword “class,” which is a grouping of object-oriented constructs.\n",
    "- A class is a code template for creating objects. Objects have member variables and have behaviour associated with them. In python a class is created by the keyword class. \n",
    "- A class by itself is of no use unless there is some functionality associated with it. Functionalities are defined by setting attributes, which act as containers for data and functions related to those attributes. Those functions are called methods.\n",
    "- You can also provide the values for the attributes at runtime. This is done by defining the attributes inside the init method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### example of a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define Haavelmo as class (base)\n",
    "# The __init__ function is a special method that is run whenever an\n",
    "# object is created. The self parameter is a reference to the current instance of the class.\n",
    "class Haavelmo():\n",
    "    \n",
    "    def __init__(self,params=None):\n",
    "               \n",
    "        # Initialize parameters\n",
    "        if params is not None:\n",
    "            self.update(params)\n",
    "            \n",
    "    def update(self, params):\n",
    "        \n",
    "        # update the parameter values during estimation  \n",
    "        self.alpha   = params[0]\n",
    "        self.beta    = params[1]\n",
    "        self.theta1  = params[2]\n",
    "        self.theta0  = params[3]\n",
    "        self.gamma1  = params[4]\n",
    "        self.gamma0  = params[5]\n",
    "        self.eps_std = params[6]\n",
    "        self.eta_std = params[7]\n",
    "        self.v_std   = params[8]    \n",
    "        \n",
    "    def Gamma_0(self):\n",
    "        Gamma0 = np.array([\n",
    "                          [1-self.alpha, -self.alpha, -self.alpha],\n",
    "                          [-self.theta1, 1, 0],\n",
    "                          [0, 0, 1],\n",
    "                          ])    \n",
    "        return Gamma0\n",
    "    \n",
    "    def C_(self):\n",
    "        C = np.array([\n",
    "                     [self.beta],\n",
    "                     [self.theta0],\n",
    "                     [self.gamma0],\n",
    "                     ])        \n",
    "        return C\n",
    "\n",
    "    def Gamma_1(self):\n",
    "        Gamma1 = np.array([\n",
    "                          [0, 0, 0],\n",
    "                          [-self.theta1, 0, 0],\n",
    "                          [0, 0, self.gamma1],\n",
    "                        ])       \n",
    "        return Gamma1\n",
    "    \n",
    "    def VV_(self):\n",
    "        VV = np.array([\n",
    "                      [self.eps_std**2, 0, 0],\n",
    "                      [0, self.eta_std**2, 0],\n",
    "                      [0, 0, self.v_std**2],\n",
    "                      ])\n",
    "        return VV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### example of a function\n",
    "The likelihood is\n",
    "\n",
    "$$f_{X^{T}}(X^{T};\\psi)=f_{X_{0}}(X_{0};\\psi)\\prod_{t=1}^{T}f_{X_{t}|X_{t-1}}\\left(X_{t}|X_{t-1};\\psi\\right)$$\n",
    "\n",
    "Then the negative loglikelihood of our model is\n",
    "\n",
    "$$\\mathcal{L}(\\psi)=-\\left(Tn/2\\right)+\\left(T/2\\right)log\\left|\\Gamma_{0}\\right|^{2}-\\left(T/2\\right)log\\left|D\\right|-0.5\\sum_{t=1}^{T}\\left[\\left(\\Gamma_{0}X_{t}-C-\\Gamma_{1}X_{t-1}\\right)'D^{-1}\\left(\\Gamma_{0}X_{t}-C-\\Gamma_{1}X_{t-1}\\right)\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_loglike(theta,endog,exog):\n",
    "    \n",
    "    '''Compute the negative of the likelihood of Haavelmo model\n",
    "       \n",
    "       Input\n",
    "       theta:the initial guesses for the parameters\n",
    "       endog:the endogenous variables\n",
    "       exog:the exogenous variables (here the lagged endogenous variables)\n",
    "       \n",
    "       Ouput\n",
    "       penalized negative loglikelihood\n",
    "       \n",
    "    '''\n",
    "    params = theta\n",
    "    mod = Haavelmo(params) # use the Haavelmo class\n",
    "    Xhat = mod.Gamma_0()@endog\n",
    "    U = Xhat - mod.C_() - mod.Gamma_1()@exog\n",
    "    T = U.shape[1]\n",
    "    n = U.shape[0]\n",
    "    temp = np.zeros(T-1)    \n",
    "    for t in range(0,T-1):\n",
    "        temp[t] = U[:,t].transpose()@np.linalg.inv(mod.VV_())@U[:,t]        \n",
    "    ll = ( -T*n/2*np.log(2*np.pi) + T/2*np.log(np.linalg.det(mod.Gamma_0()))\n",
    "           -0.5*temp.sum() - T/2*np.log(np.linalg.det(mod.VV_())) )                 \n",
    "    ll = ( ll - 1000*min(0,1 -params[0]*(1+params[2]))**2 \n",
    "              - 1000*min(0,params[2])**2\n",
    "              - 1000*min(0,params[6])**2\n",
    "              - 1000*min(0,params[7])**2\n",
    "              - 1000*min(0,params[8])**2 )   \n",
    "    return -ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Prepare the data X and X(-1) this would be the Haavelmo original sample\n",
    "'''\n",
    "endog = np.array(haavelmo_data[['c','i','g']]).T  \n",
    "endog = endog[:,1:] # drop first observation because of one lag\n",
    "\n",
    "exog = np.array(haavelmo_data[['c','i','g']].shift(1)).T      \n",
    "exog = exog[:,1:]   # drop first observation because of one lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Prepare the data DlogX and DlogX(-1)\n",
    "We will use the growth rate to be consistent with stationarity\n",
    "'''\n",
    "\n",
    "haavelmo_data['lc'] = np.log(haavelmo_data['c'])\n",
    "haavelmo_data['li'] = np.log(haavelmo_data['i'])\n",
    "haavelmo_data['lg'] = np.log(haavelmo_data['g'])\n",
    "\n",
    "haavelmo_data['dlc'] = haavelmo_data['lc'] - haavelmo_data['lc'].shift(1)\n",
    "haavelmo_data['dli'] = haavelmo_data['li'] - haavelmo_data['li'].shift(1)\n",
    "haavelmo_data['dlg'] = haavelmo_data['lg'] - haavelmo_data['lg'].shift(1)\n",
    "\n",
    "endog = np.array(haavelmo_data[['dlc','dli','dlg']]).T       \n",
    "endog = endog[:,2:] # drop first 2 observations because of one lag\n",
    "\n",
    "exog  = np.array(haavelmo_data[['dlc','dli','dlg']].shift(1)).T       \n",
    "exog  = exog[:,2:]   # drop first 2 observations because of one lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data in growth rate\n",
    "plt.figure(figsize=(16,10))\n",
    "plt.subplot(221)\n",
    "plt.plot(haavelmo_data.dlc)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Billion Chained Dollar')\n",
    "plt.title('Consumption')\n",
    "plt.grid()\n",
    "plt.subplot(222)\n",
    "plt.plot(haavelmo_data.dli)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Billion Chained Dollar')\n",
    "plt.title('Investment')\n",
    "plt.grid()\n",
    "plt.subplot(223)\n",
    "plt.plot(haavelmo_data.dlg)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Billion Chained Dollar')\n",
    "plt.title('Government expenditure')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimization using Scipy package routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Minimization of -LogLikelihood, constrained\n",
    "'''\n",
    "# initial guesses        alpha  beta  theta1 theta0 gamma1 gamma0 eps_std eta_std v_std        \n",
    "theta_start = np.array([ 0.05, 0.5,  0.05,   0.5,  0.991,   0.2,     0.1,   .1,     .1])\n",
    "# Nelder-Mead, BFGS, \n",
    "res = minimize(neg_loglike, theta_start,args=(endog,exog),method=\"BFGS\",\n",
    "               options={'disp': True,'maxiter':100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Estimated parameters\n",
    "'''\n",
    "sol = pd.DataFrame(res.x,index=[r'$\\alpha$',r'$\\beta$', r'$\\theta_1$',\n",
    "                                r'$\\theta_0$',r'$\\gamma_1$',r'$\\gamma_0$',\n",
    "                                r'$\\sigma_{\\epsilon}$',r'$\\sigma_{\\eta}$',r'$\\sigma_{\\nu}$'],columns=['parameters'])\n",
    "sol['SE']=np.sqrt(np.diagonal(res.hess_inv))\n",
    "sol['T-stat']=sol['parameters']/sol['SE']\n",
    "sol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder to interpret the parameters\n",
    "\n",
    "$$C_t = \\beta + {\\alpha}Y_t+\\epsilon_t$$\n",
    "\n",
    "$$I_t = \\theta_0 + \\theta_1(C_t - C_{t-1}) + \\eta_t$$\n",
    "\n",
    "$$Y_t = C_t + I_t + G_t$$\n",
    "\n",
    "$$G_t = \\gamma_0 + \\gamma_1G_{t-1} + v_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice and common way to highlight the differences between frequentist and bayesian analysis is to compare the confidence interval of frequentists with the corresponding notion in Bayesian statistics, credible interval.\n",
    "\n",
    "- Confidence interval is from the frequentist's approach, where the parameter is fixed. The confidence interval is based on the repetition of the observations. A 95% confidence interval means that repeating the experiment to measure the parameter a large number of times and calculating the interval for each experiment, 95% of the intervals will contain the value of the parameter. This goes back to the fact that the data is random.\n",
    "\n",
    "\n",
    "- Credible (or probability) interval stems from probabilities, that is, the Bayesian approach. This means that the parameter is random and we can say that, given the data, there is a 95% chance that the true value of the parameter is in the interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov chain Monte Carlo (MCMC) methods\n",
    "In Bayesian statistics the parameters are not constant but random variables with a distribution. We are now looking for\n",
    "\n",
    "$$\\pi(\\psi|X^T)= \\frac{f_{X^T}(X^T|\\psi)\\pi(\\psi)}{f_{X^T}(X^T)}$$\n",
    "\n",
    "where the first term of the denominator on the RHS is the likelihood and the second the prior distribution of the parameters.The LHS is the posterior distribution and is the quantity of interest. MCMC methods allow to sample from ths posterior distribution.\n",
    "\n",
    "One of the most simple algorithm to implement a MCMC is the Metropolis-Hastings algorithm. The idea is to construct a Markov chain for $\\psi$\n",
    "\n",
    "1. Given the current value of $\\psi_{s-1}$, propose a new value $\\psi^*$ selected from a proposal $q(\\psi;\\psi_{s-1})$\n",
    "\n",
    "2. With probability $\\alpha(\\psi_{s-1},\\psi^*)$ the proposed value is accepted if not the chain remains in place\n",
    "now $$\\alpha(\\psi_{s-1},\\psi^*) = min\\Big({\\frac{\\pi(\\psi^*|X^T)q(\\psi^*;\\psi_{s-1})}{\\pi(\\psi_{s-1}|X^T)q(\\psi_{s-1};\\psi^*)}},1\\Big)$$\n",
    "\n",
    " $$  \\alpha(\\psi_{s-1},\\psi^*)                                    = min\\Big({\\frac{f_{X^T}(X^T|\\psi^*)\\pi(\\psi^*)q(\\psi^*;\\psi_{s-1})}{f_{X^T}(X_T|\\psi_{s-1})\\pi(\\psi_{s-1})q(\\psi_{s-1};\\psi_{s-1}^*)}},1\\Big)$$\n",
    " \n",
    " and using a proposal distribution that satistifes $q(\\psi_{s-1};\\psi_{s-1}^*)=q(\\psi^*;\\psi_{s-1})$. A convienent choice is\n",
    " \n",
    " $$\\psi^*=\\psi_{s-1} +\\epsilon_s, \\epsilon_s \\sim{N(0,\\Sigma_\\epsilon)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_alpha = uniform(loc =0, scale=3)   # Specify priors\n",
    "prior_beta = uniform(loc =0, scale=3)        \n",
    "prior_theta = uniform(loc =0, scale=3)         \n",
    "prior_theta0 = uniform(loc =0, scale=3)         \n",
    "prior_gamma1 = uniform(loc =0, scale=2)         \n",
    "prior_gamma0 = uniform(loc =0, scale=2) \n",
    "prior_eps_std = invgamma(5)\n",
    "prior_eta_std = invgamma(5)\n",
    "prior_v_std  = invgamma(5)\n",
    "\n",
    "rw_proposal = multivariate_normal(cov=res.hess_inv*0.4)   # Specify the random walk proposal\n",
    "                                                          # an important aspect: the initial covariance matrix is\n",
    "                                                          # the inverse of the Hessian of likelihod and the parameter\n",
    "                                                          # is set as to obtain a 25% acceptance rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Metropolis Hastings algorithm\n",
    "'''\n",
    "\n",
    "n_iterations = 1000\n",
    "trace = np.zeros((n_iterations + 1, 9)) # Create storage arrays for the traces\n",
    "trace_accepts = np.zeros(n_iterations)\n",
    "trace[0] = [0.05, 0.5, 1, 0.5, 0.991, 0,0.1,.1,.1] # Initial values\n",
    "# Iterations\n",
    "for s in range(1, n_iterations + 1):\n",
    "    proposed = trace[s-1] + rw_proposal.rvs() # step 1\n",
    "    acceptance_probability = np.exp(-neg_loglike(proposed,endog,exog) + neg_loglike(trace[s-1],endog,exog) +\n",
    "    prior_alpha.logpdf(proposed[0]) +\n",
    "    prior_beta.logpdf(proposed[1]) + prior_theta.logpdf(proposed[2]) +\n",
    "    prior_theta0.logpdf(proposed[3])+ prior_gamma1.logpdf(proposed[4]) +\n",
    "    prior_gamma0.logpdf(proposed[5])+ prior_eps_std.logpdf(proposed[6]) + \n",
    "    prior_eps_std.logpdf(proposed[7]) + prior_eps_std.logpdf(proposed[8]) -\n",
    "    prior_alpha.logpdf(trace[s-1, 0]) -\n",
    "    prior_beta.logpdf(trace[s-1, 1]) - prior_theta.logpdf(trace[s-1, 2]) -\n",
    "    prior_theta0.logpdf(trace[s-1, 3])- prior_gamma1.logpdf(trace[s-1, 4]) -\n",
    "    prior_gamma0.logpdf(trace[s-1, 5])- prior_eps_std.logpdf(trace[s-1, 6]) - \n",
    "    prior_eps_std.logpdf(trace[s-1, 7]) - prior_eps_std.logpdf(trace[s-1, 8])) # compute odd ratio\n",
    "    pp = uniform.rvs()                                \n",
    "    if acceptance_probability > pp: # step 2\n",
    "        trace[s] = proposed\n",
    "        trace_accepts[s-1] = 1\n",
    "    else:\n",
    "        trace[s] = trace[s-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(trace_accepts)/np.arange(1, len(trace_accepts)+1)) # Plot the acceptance rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trace[:,4]) # plot the value sample gamma1 from the posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(trace[:,4]) # Plot the histogram of gamma1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=['alpha','beta', 'theta1','theta0','gamma1','gamma0','eps_std','eta_std','v_std']\n",
    "fig, axes = plt.subplots(9, 2, sharex='col')\n",
    "fig.set_size_inches(12, 40)\n",
    "for i in range(9):\n",
    "  axes[i][0].plot(trace[:,i])\n",
    "  axes[i][0].title.set_text(str(index[i]))\n",
    "  sns.kdeplot(trace[:,i], ax=axes[i][1], shade=True)\n",
    "  axes[i][1].title.set_text(str(index[i]))\n",
    "axes[9 - 1][0].set_xlabel(\"Iteration\")\n",
    "axes[9 - 1][1].set_xlabel(\"coefficient\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
