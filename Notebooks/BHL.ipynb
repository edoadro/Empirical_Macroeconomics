{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMPIRICAL MACROECONOMICS¶\n",
    "\n",
    "Francesco Franco, Nova SBE\n",
    "\n",
    "## Non recoverable shocks: replication of Blanchard, Olivier J., Jean-Paul L'Huillier, and Guido Lorenzoni AER 2013\n",
    "**News, Noise, and Fluctuations: An Empirical Exploration.\" American Economic Review, 103 (7): 3045-70.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART I The Economic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The optimal consumption decision of the consumer requires to solve a filtering problem: \n",
    "$$S_{t}^{e}=\\left[\\begin{array}{c}\n",
    "x_{t}\\\\\n",
    "x_{t-1}\\\\\n",
    "z_{t}\n",
    "\\end{array}\\right]=\\begin{bmatrix}1+\\rho & -\\rho & 0\\\\\n",
    "1 & 0 & 0\\\\\n",
    "0 & 0 & \\rho\n",
    "\\end{bmatrix}S_{t-1}^{e}+\\begin{bmatrix}1 & 0\\\\\n",
    "0 & 0\\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\\left[\\begin{array}{c}\n",
    "\\epsilon_{t}\\\\\n",
    "\\eta_{t}\n",
    "\\end{array}\\right]$$\n",
    "$$X_{t}^{e}=\\begin{bmatrix}a_{t}\\\\\n",
    "s_{t}\n",
    "\\end{bmatrix}=\\begin{bmatrix}1 & 0 & 1\\\\\n",
    "1 & 0 & 0\n",
    "\\end{bmatrix}S_{t}^{e}+\\begin{bmatrix}0\\\\\n",
    "1\n",
    "\\end{bmatrix}v_{t}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Importing packages and functions''' \n",
    "\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "from scipy.optimize import root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Build the Matrices od the SSM'''\n",
    "\n",
    "rho      = 0.891\n",
    "sig2_a   = 0.67**2\n",
    "sig2_nu  = 2.89**2\n",
    "sig2_eps = (1-rho)**2*sig2_a\n",
    "sig2_eta = sig2_a*rho\n",
    "\n",
    "Q = np.array([[sig2_eps,0,0],\n",
    "              [0,0,0],\n",
    "              [0,0,sig2_eta]])\n",
    "\n",
    "A = np.array([[1+rho,-rho,0],\n",
    "              [1,0,0],\n",
    "              [0,0,rho]])\n",
    "\n",
    "B =np.array([[1,0],\n",
    "             [0,0],\n",
    "             [0,1]])\n",
    "\n",
    "C = np.array([[1,0,1],\n",
    "              [1,0,0]])\n",
    "\n",
    "D = np.array([[0],\n",
    "              [1]])\n",
    "\n",
    "R = np.array([[0,0],\n",
    "              [0,sig2_nu]])\n",
    "\n",
    "Σ_t_t1 = np.array([[99999,0,0],\n",
    "                   [0,99999,0],\n",
    "                   [0,0,sig2_eta/(1-rho)]])\n",
    "Σ_t_t1 = np.array([[1,0,0],\n",
    "                   [0,1,0],\n",
    "                   [0,0,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The consumer forms expectations on the productivity process\n",
    "$$\\hat{S}_{t|t}=\\hat{S}_{t|t-1}+\\Sigma_{t,t-1}C'\\left[C\\Sigma_{t,t-1}C'+R\\right]^{-1}\\left(X_{t}-C\\hat{S}_{t|t-1}\\right)$$\n",
    "Define $$K_t =\\Sigma_{t,t-1}C'\\left[C\\Sigma_{t,t-1}C'+R\\right]^{-1}$$ and $$\\Sigma_{t|t}=\\Sigma_{t|t-1}-K_t\\Sigma_{t|t-1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "compute the Kalman gain, stationary assuming many periods have passed\n",
    "'''\n",
    "iter = 40\n",
    "\n",
    "conv = np.zeros(iter) \n",
    "for i in range(1,iter):\n",
    "    \n",
    "    K      = Σ_t_t1@C.T@LA.inv(C@(Σ_t_t1@C.T)+R)\n",
    "    Σ_t_t  = Σ_t_t1 - K@C@Σ_t_t1\n",
    "    Σ_t_t1 = A@Σ_t_t@A.T + Q\n",
    "    conv[i] = K[1,1]\n",
    "plt.plot([i for i in range(iter)],conv)    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with expectations formed the consumer uses the expectations to compute his consumption decision\n",
    "we just obtained the law of motion of the expecations:\n",
    "$$\\hat{S}_{t|t}=\\hat{S}_{t|t-1}+K\\left(X_{t}-C\\hat{S}_{t|t-1}\\right)$$\n",
    "\n",
    "$$\\hat{S}_{t|t}=A\\hat{S}_{t-1|t-1}+K\\left(X_{t}-CA\\hat{S}_{t-1|t-1}\\right)$$\n",
    "\n",
    "$$\\hat{S}_{t|t}=(I-KC)A\\hat{S}_{t-1|t-1}+KX_{t}$$\n",
    "then using $$X_{t}=CS_{t}+Dv_{t}$$ and $$S_{t}=AS_{t-1}+B\\eta_{t}$$\n",
    "\n",
    "you obtain the complete solution for the consumer:\n",
    "\n",
    "$$S_{t}^{e}=\\left[\\begin{array}{c}\n",
    "x_{t}\\\\\n",
    "x_{t-1}\\\\\n",
    "z_{t}\\\\\n",
    "\\hat{x}_{t|t}\\\\\n",
    "\\hat{x}_{t-1|t-1}\\\\\n",
    "\\hat{z}_{t|t}\n",
    "\\end{array}\\right]=\\begin{bmatrix}A & 0\\\\\n",
    "KCA & \\left(I-KC\\right)A\n",
    "\\end{bmatrix}S_{t-1}^{e}+\\begin{bmatrix}B & 0\\\\\n",
    "KCB & KD\n",
    "\\end{bmatrix}\\left[\\begin{array}{c}\n",
    "\\epsilon_{t}\\\\\n",
    "\\eta_{t}\\\\\n",
    "v_{t}\n",
    "\\end{array}\\right]$$\n",
    "$$X_{t}^{e}=\\begin{bmatrix}a_{t}\\\\\n",
    "c_{t}\n",
    "\\end{bmatrix}=\\begin{bmatrix}1 & 0 & 1 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 0 & \\frac{1}{1-\\rho} & -\\frac{\\rho}{1-\\rho} & 0\n",
    "\\end{bmatrix}S_{t}^{e}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Build the matrices of the consumer model\n",
    "'''\n",
    "IKCA  = (np.identity(3)-K@C)@A\n",
    "BIG_A = np.vstack([np.hstack([A,np.zeros([3,3])]),\n",
    "                   np.hstack([K@C@A,IKCA])])     \n",
    "BIG_B = np.vstack([np.hstack([B,np.zeros([3,1])]),\n",
    "                   np.hstack([K@C@B,K@D])])\n",
    "BIG_C = np.vstack([[1,0,1,0,0,0],\n",
    "                  [0,0,0,1/(1-rho),-rho/(1-rho),0]])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "SIMULATE a PERMANENT SHOCK\n",
    "'''\n",
    "T=30\n",
    "shocks = np.atleast_2d(np.array([sig2_eps**0.5, 0, 0]))\n",
    "S = BIG_B@shocks.T\n",
    "c_eps = np.zeros([T,1])\n",
    "a_eps = np.zeros([T,1])\n",
    "for t in range(0,T):\n",
    "    c_eps[t]=[0,0,0,1/(1-rho),-rho/(1-rho),0]@S\n",
    "    a_eps[t]=[1,0,1,0,0,0]@S\n",
    "    S=BIG_A@S\n",
    "plt.title('Permanent shock')\n",
    "\n",
    "plt.plot(c_eps)\n",
    "plt.plot(a_eps)\n",
    "plt.legend(('Consumption', 'Productivity'),loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "SIMULATE A TRANSITORY SHOCK\n",
    "'''\n",
    "T=30\n",
    "shocks = np.atleast_2d(np.array([0, sig2_eta**0.5, 0]))\n",
    "S = BIG_B@shocks.T\n",
    "c_eta = np.zeros([T,1])\n",
    "a_eta = np.zeros([T,1])\n",
    "for t in range(0,T):\n",
    "    c_eta[t]=[0,0,0,1/(1-rho),-rho/(1-rho),0]@S\n",
    "    a_eta[t]=[1,0,1,0,0,0]@S\n",
    "    S=BIG_A@S\n",
    "plt.title('Transitory shock')\n",
    "\n",
    "plt.plot(c_eta)\n",
    "plt.plot(a_eta)    \n",
    "plt.legend(('Consumption', 'Productivity'),loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "SIMULATE A Noise SHOCK\n",
    "'''\n",
    "T=30\n",
    "shocks = np.atleast_2d(np.array([0, 0, sig2_nu**0.5]))\n",
    "S = BIG_B@shocks.T\n",
    "c_nu = np.zeros([T,1])\n",
    "a_nu = np.zeros([T,1])\n",
    "for t in range(0,T):\n",
    "    c_nu[t]=[0,0,0,1/(1-rho),-rho/(1-rho),0]@S\n",
    "    a_nu[t]=[1,0,1,0,0,0]@S\n",
    "    S=BIG_A@S\n",
    "plt.title('Noise shock')\n",
    "\n",
    "plt.plot(c_nu)\n",
    "plt.plot(a_nu)    \n",
    "plt.legend(('Consumption', 'Productivity'),loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART II The SVAR\n",
    "The econometrician will not be able to recover the shocks without estimating the structural model. To coninvce ourselves we will proceed with an artificial experiment:\n",
    "1. Assume the toy-model is the true model and generate an artificial dataset\n",
    "2. The long-run versus short-run dichotomy suggests we apply the Blanchard and Quah decomposition in a SVAR\n",
    "3. We can now compare the IRF from the model (the true IRF) with teh IRF from the SVAR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Simulate Artificial data from the model'''\n",
    "T_obs  = 500\n",
    "S_sim  = np.zeros([6,1])\n",
    "c_sim  = np.zeros([T_obs,1])\n",
    "a_sim  = np.zeros([T_obs,1])\n",
    "dc_sim = np.zeros([T_obs+1,1])\n",
    "da_sim = np.zeros([T_obs+1,1])\n",
    "scale  = np.atleast_2d(np.array([sig2_eps**0.5, sig2_eta**0.5, sig2_nu**0.5]))\n",
    "np.random.seed(1234)\n",
    "for t in range(1,T_obs):\n",
    "    \n",
    "    shocks    = scale.T*np.random.randn(3,1)\n",
    "    S_sim     = BIG_A@S_sim + BIG_B@shocks\n",
    "    c_sim[t]  = [0,0,0,1/(1-rho),-rho/(1-rho),0]@S_sim\n",
    "    a_sim[t]  = [1,0,1,0,0,0]@S_sim\n",
    "    dc_sim[t] = c_sim[t] - c_sim[t-1]\n",
    "    da_sim[t] = a_sim[t] - a_sim[t-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Let us plot the artificial data'''\n",
    "plt.plot(a_sim)\n",
    "plt.plot(c_sim)\n",
    "plt.legend(('Productivity','Consumption'))\n",
    "plt.show()\n",
    "print('They are clearly co-integrated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the problem set you have derived the VAR representation of the model:\n",
    "$$c_{t}\t=c_{t-1}+u_{t}^{c}$$\n",
    "$$a_{t}\t=\\rho a_{t-1}+(1-\\rho)c_{t-1}+u_{t}^{a}$$\n",
    "Subtract $a_t$ from the second equation and you obtain that $$\\Delta a_{t}=\\left(1-\\rho\\right)\\left(c_{t-1}-a_{t-1}\\right)+u_{t}^{a}$$Given $\\Delta a_{t}$ is stationary, this implies that consumption and productivity are cointegrated with a vector $(1,-1)$\n",
    "### Let us estimate the cointegration relationship\n",
    "If two time-series are cointegrated the regression: $$a_t = \\beta + \\beta_cc_t + u_t$$\n",
    "is legitimate (actually the $\\beta_c$ is superconsistent) and the residuals $u_t$ are stationary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Cointegrating vector'''\n",
    "reg        = np.hstack([np.ones([c_sim.shape[0],c_sim.shape[1]]),c_sim])\n",
    "bhat_c     = LA.inv(reg.T@reg)@reg.T@a_sim\n",
    "u_t        = a_sim - reg@bhat_c\n",
    "da_co      = (a_sim - c_sim)\n",
    "plt.plot(u_t)\n",
    "plt.plot(da_co)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVAR with LR when there is cointegration (* extra material not for the exam)\n",
    "There are two cases:\n",
    "1. If the $I(1)$ variables are not cointegrated or the cointegrating relations are known, then all I(1) variables may be transformed to $I(0)$ variables by expressing them in first differences or as cointegration relations. In that case we just use usual identification scheme.\n",
    "2. An alternative and more general reduced-form representation of the VAR  that also allows for unknown cointegrating relations is the VECM \n",
    "\n",
    "$$\\Delta X_{t}=\\Pi X_{t-1}+\\Gamma_{1}\\Delta X_{t-1}+...+\\Gamma_{p-1}\\Delta X_{t-p+1}+u_{t}$$\n",
    "\n",
    "where $\\Pi=\\alpha\\beta'$\n",
    "\n",
    "using the Granger representation we can show that the long run effect matrix of the reduced form is in this case\n",
    "\n",
    "$$\\Xi=\\beta_{\\perp}\\left[\\alpha'_{\\perp}\\left(I_{2}-\\sum_{i=1}^{p-1}\\Gamma_{i}\\right)\\beta_{\\perp}\\right]^{-1}\\alpha'_{\\perp}$$\n",
    "and the structural long run matrix is (using $u_{t}=A_{0}\\eta_{t}$) $$LR=\\Xi A_{0}$$\n",
    "\n",
    "that you can use to impose the long run restrition.\n",
    "\n",
    "You can also use the fact that you can recast the VEC into a VAR in levels and then use the standard IRF formulas. To recast the VEC into a VAR form\n",
    "\n",
    "$$X_{t}=A_{1}X_{t-1}+...+A_{p}X_{t-p}+u_{t}$$\n",
    "\n",
    "you can use the following $$A_{1}=\\Pi+\\Gamma_{1}+I$$ $$A_{i}=\\Gamma_{i}-\\Gamma_{i-1}\\ for\\ i=2...p-1$$\n",
    "$$A_{p}=-\\Gamma_{p-1}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Build the VEC DATA'''\n",
    "\n",
    "df   = np.hstack([dc_sim,da_sim])\n",
    "X    = pd.DataFrame(df)\n",
    "df2  = pd.DataFrame(da_co)\n",
    "XLAG = pd.DataFrame()\n",
    "\n",
    "num_lags = 4 \n",
    "\n",
    "for i in range(1,num_lags+1):\n",
    "    XLAG = pd.concat([XLAG,X.shift(i).add_suffix(\"-\"+str(i))],axis=1)\n",
    "\n",
    "#change names to frames that we modify   \n",
    "XLAG     = pd.concat([df2,XLAG],axis=1)\n",
    "X2       = X.iloc[num_lags:T_obs-2,:]\n",
    "XLAG2    = XLAG.iloc[num_lags:T_obs-2,:]\n",
    "num_vars = X2.shape[1]\n",
    "num_obs  = XLAG2.shape[0]\n",
    "#Building arrays for using OLS\n",
    "X3       = np.array(X2)\n",
    "XLAG3    = np.array(XLAG2)\n",
    "#VAR - standard OLS\n",
    "Ahat     = LA.inv(XLAG3.T@XLAG3)@XLAG3.T@X3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ahat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estimated errors\n",
    "EPS  = X3-XLAG3@Ahat\n",
    "#estimated covariance matrix\n",
    "Vhat = EPS.T@EPS/(num_obs - num_lags*num_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the LR matrix:  $\\Xi=\\beta_{\\perp}\\left[\\alpha'_{\\perp}\\left(I_{2}-\\sum_{i=1}^{p-1}\\Gamma_{i}\\right)\\beta_{\\perp}\\right]^{-1}\\alpha'_{\\perp}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bulding the LR matrix \n",
    "Γ        = Ahat[1:,:].T\n",
    "Γ1       = Γ[:,0:2]\n",
    "Γ2       = Γ[:,2:4]\n",
    "Γ3       = Γ[:,4:6]\n",
    "Γ4       = Γ[:,6:8]\n",
    "a        = np.hstack([Ahat[0,:].reshape(2,1), - Ahat[0,:].reshape(2,1)])\n",
    "b        = np.array([[1,-1],[-1,1]]) # cointegration relationship could be estimated\n",
    "u, s, vh = LA.svd(a, full_matrices=True)\n",
    "a_ht     = u[0:2,1].reshape(2,1)\n",
    "u, s, vh = LA.svd(b, full_matrices=True)\n",
    "b_ht     = u[0:2,1].reshape(2,1)\n",
    "Ξ        = b_ht@LA.inv(a_ht.T@(np.identity(2)- Γ1-Γ2-Γ3-Γ4)@b_ht)@a_ht.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long run restrictions\n",
    "def objective(x0,Ξ,Vhat,num_vars): \n",
    "        A0     = np.reshape(x0,[num_vars,num_vars]) \n",
    "        LR     = Ξ@A0 \n",
    "        H      = A0@A0.T - Vhat \n",
    "        H[0,1] = LR[0,1] \n",
    "        H      = H.flatten()\n",
    "        return H "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the structural shock accroding to the LR identification\n",
    "\n",
    "x0  = ((Vhat**2)**.25).flatten() \n",
    "sol = root(objective,x0,args=(Ξ,Vhat,num_vars))\n",
    "A0  = sol.x.reshape([num_vars,num_vars]) \n",
    "\n",
    "#the first shock (supply) should have a positive impact on output (only redefines direction, and changes nothing)\n",
    "if A0[0,0] < 0: A0[:,0] = -A0[:,0]\n",
    "#the second shock (demand) should have a positive impact on output (only redefines direction, and changes nothing)\n",
    "if A0[0,1] < 0: A0[:,1] = -A0[:,1]\n",
    "#Estimating structural shocks, ETA\n",
    "F   = LA.inv(A0)\n",
    "ETA = EPS@F.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast the estimated VAR in levels\n",
    "A1 = a@b + Γ1 + np.identity(2)\n",
    "A2 = Γ2-Γ1\n",
    "A3 = Γ3-Γ2\n",
    "A4 = Γ4-Γ3\n",
    "A5 = -Γ4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_Ahat2 = np.vstack((np.hstack([A1,A2,A3,A4,A5]),np.hstack((np.identity((num_lags)*num_vars),np.zeros([(num_lags)*num_vars,num_vars]))))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IRFs\n",
    "'''IRFs are stored in a 3-dimensional array. Dimension 1 is time. Dimension\n",
    "    2 is variable, and 3 is shock. So IRF(:,2,1) gives the impulse response\n",
    "    of the second variable to the first shock. IRF_sum gives the integrated\n",
    "    responses (i.e, in levels). IRF_sum(:,1,1) gives the cumulative impulse\n",
    "    response of variable 1 to shock 1.'''\n",
    "    \n",
    "num_impulses = 30\n",
    "IRF = np.zeros([num_impulses,num_vars,num_vars])\n",
    "Temp = np.identity(c_Ahat2.shape[0])\n",
    "\n",
    "psi = []\n",
    "for t in range(num_impulses):\n",
    "    psi_t = Temp[:num_vars,:num_vars] \n",
    "    IRF[t,:,:] = np.dot(psi_t,A0) # store the IRF\n",
    "    Temp = np.dot(c_Ahat2,Temp)   # computes the exponent of the matrix\n",
    "    #psi.append(psi_t)            # stores the matrices Psi (notations as in Hamilton)\n",
    "\n",
    "irf = pd.DataFrame({i:IRF[i].flatten() for i in range(num_impulses)}).T #save IRFs into dataframe\n",
    "irf.columns = [\"p_c\",\"t_c\",\"p_ac\",\"t_ac\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(16,10))\n",
    "plt.subplot(221)\n",
    "plt.plot(irf[\"p_c\"],label='permanent shock on consumption')\n",
    "plt.plot(c_eps,label='permanent shock on consumption from model')\n",
    "plt.legend()\n",
    "plt.xlabel('period')\n",
    "plt.grid()\n",
    "plt.subplot(222)\n",
    "plt.plot(irf[\"p_ac\"],label='permanent shock on productivity')\n",
    "plt.plot(a_eps,label='permanent shock on productivity from model')\n",
    "plt.legend()\n",
    "plt.xlabel('period')\n",
    "plt.grid()\n",
    "plt.subplot(223)\n",
    "plt.plot(irf[\"t_c\"],label='transitory shock on consumption')\n",
    "plt.plot(c_eta,label='transitory shock on consumption from model')\n",
    "plt.plot(c_nu,label='noise shock on consumption from model')\n",
    "plt.legend()\n",
    "plt.xlabel('period')\n",
    "plt.grid()\n",
    "plt.subplot(224)\n",
    "plt.plot(irf[\"t_ac\"],label='transitory shock on productivity')\n",
    "plt.plot(a_eta,label='transitory shock on productivity from model')\n",
    "plt.plot(a_nu,label='noise shock on productivity from model')\n",
    "plt.legend()\n",
    "plt.xlabel('period')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART III The maximum Likelihood estimation\n",
    "The econometrician states variables are $S_{t}^{e}=\\left(S_{t},\\hat{S}_{t|t}\\right)'$ and the observalbes $X_{t}^{e}=\\left(a_{t},c_{t}\\right)$ which deliver the following SSM to be estimated (namely the model economy)\n",
    "$$S_{t}^{e}=\\left[\\begin{array}{c}\n",
    "x_{t}\\\\\n",
    "x_{t-1}\\\\\n",
    "z_{t}\\\\\n",
    "\\hat{x}_{t|t}\\\\\n",
    "\\hat{x}_{t-1|t-1}\\\\\n",
    "\\hat{z}_{t|t}\n",
    "\\end{array}\\right]=\\begin{bmatrix}A & 0 & 0\\\\\n",
    "KCA & \\left(I-KC\\right)A & 0\n",
    "\\end{bmatrix}S_{t-1}^{e}+\\begin{bmatrix}B & 0\\\\\n",
    "KCB & KD\n",
    "\\end{bmatrix}\\begin{bmatrix}\\epsilon_{t}\\\\\n",
    "\\eta_{t}\\\\\n",
    "v_{t}\n",
    "\\end{bmatrix}$$\n",
    "and $$X_{t}^{e}=\\begin{bmatrix}a_{t}\\\\\n",
    "c_{t}\n",
    "\\end{bmatrix}=\\begin{bmatrix}1 & 0 & 1 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 0 & \\frac{1}{1-\\rho} & -\\frac{\\rho}{1-\\rho} & 0 & 0\n",
    "\\end{bmatrix}S_{t}^{e}$$\n",
    "Let us build a method to build the model BHL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BHL():\n",
    "    '''method to compute the BHL model'''    \n",
    "    def __init__(self, params=None):\n",
    "                  \n",
    "        # Initialize parameters\n",
    "        if params is not None:\n",
    "            self.update(params)\n",
    "            \n",
    "    def update(self, params):\n",
    "        \n",
    "        # update the parameter values during estimation  \n",
    "        self.rho      = params[0]\n",
    "        self.sig2_a   = params[1]\n",
    "        self.sig2_nu  = params[2]\n",
    "        self.sig2_eps = (1-params[0])**2*params[1]\n",
    "        self.sig2_eta = params[0]*params[1]\n",
    "        \n",
    "    def K(self):\n",
    "        # compute the Kalman gain for the consumer filtering problem\n",
    "        # and the associated filtering matrices for the computation\n",
    "        # of the consumer expectations\n",
    "        \n",
    "        Q = np.array([[self.sig2_eps,0,0],\n",
    "                      [0,0,0],\n",
    "                      [0,0,self.sig2_eta]])\n",
    "        A = np.array([[1+self.rho,-self.rho,0],\n",
    "                      [1,0,0],\n",
    "                      [0,0,self.rho]])\n",
    "        B = np.array([[1,0],\n",
    "                      [0,0],\n",
    "                      [0,1]])\n",
    "        C = np.array([[1,0,1],\n",
    "                      [1,0,0]])\n",
    "        D = np.array([[0],\n",
    "                      [1]])\n",
    "        R = np.array([[0,0],\n",
    "                      [0,self.sig2_nu]])\n",
    "        \n",
    "        #Σ_t_t1a = np.array([[99999,0,0],\n",
    "        #                    [0,99999,0],\n",
    "        #                    [0,0,sig2_eta/(1-rho)]])\n",
    "        \n",
    "        Σ_t_t1a = np.array([[1,0,0],\n",
    "                            [0,1,0],\n",
    "                            [0,0,1]])\n",
    "    \n",
    "        iter = 100\n",
    "        for i in range(1,iter):\n",
    "           \n",
    "            K      = Σ_t_t1a@C.T@LA.inv(C@(Σ_t_t1a@C.T)+R)\n",
    "            Σ_t_ta  = Σ_t_t1a - K@C@Σ_t_t1a\n",
    "            Σ_t_t1a = A@Σ_t_ta@A.T + Q\n",
    "        \n",
    "                \n",
    "        \n",
    "        IKCA   = (np.identity(3)-K@C)@A\n",
    "        BIG_Ae = np.vstack([np.hstack([A,np.zeros([3,3])]),\n",
    "                            np.hstack([K@C@A,IKCA])])     \n",
    "        BIG_Be = np.vstack([np.hstack([B,np.zeros([3,1])]),\n",
    "                            np.hstack([K@C@B,K@D])])\n",
    "        BIG_Ce = np.vstack([[1,0,1,0,0,0],\n",
    "                            [0,0,0,1/(1-rho),-rho/(1-rho),0]])\n",
    " \n",
    "        Qe     = np.array([[self.sig2_eps,0,0],\n",
    "                           [0,self.sig2_eta,0],\n",
    "                           [0,0,self.sig2_nu]])\n",
    "        \n",
    "        \n",
    "        return IKCA, BIG_Ae, BIG_Be, BIG_Ce, Qe\n",
    "    \n",
    "    \n",
    "    def Q(self):\n",
    "        \n",
    "        Q = np.array([[self.sig2_eps,0,0],\n",
    "                      [0,0,0],\n",
    "                      [0,0,self.sig2_eta]])\n",
    "        return Q\n",
    "    \n",
    "    def A(self):\n",
    "        \n",
    "        A = np.array([[1+self.rho,-self.rho,0],\n",
    "                      [1,0,0],\n",
    "                      [0,0,self.rho]])\n",
    "        return A\n",
    "    \n",
    "    def B(self):\n",
    "    \n",
    "        B =np.array([[1,0],\n",
    "                     [0,0],\n",
    "                     [0,1]])\n",
    "        return B\n",
    "    \n",
    "    def C(self):\n",
    "        C = np.array([[1,0,1],\n",
    "                      [1,0,0]])\n",
    "        return C\n",
    "    \n",
    "    def D(self):\n",
    "        D = np.array([[0],\n",
    "                      [1]])\n",
    "        return D\n",
    "    \n",
    "    def R(self):\n",
    "        R = np.array([[0,0],\n",
    "                      [0,self.sig2_nu]])\n",
    "        return R\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily compute the likelihood. If the inital state $S_{1}$ and $\\eta_{t}$ and $v_{t}$ are multivariate Gaussian we have:$$f(X_{t}|X_{t-1},X_{t-2},...)\\sim N\\left(C\\hat{S}_{t|t-1},C\\Sigma_{t|t-1}C'+D\\Sigma_{v}D'\\right)\\,for\\,t=1,2,...,T$$from which you can compute the sample loh likelihhod and maximize wrt to $A,B,C,D,\\Sigma,\\Sigma_{v}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_loglike(theta,endog):\n",
    "    r=6\n",
    "    n=2\n",
    "    T             = endog.shape[1]\n",
    "    S_t_t1        = np.zeros([T,r,1])\n",
    "    S_t_t         = np.zeros([T,r,1])\n",
    "    X_t_t1        = np.zeros([T,n,1])\n",
    "    Σ_t_t1        = np.zeros([T,r,r])\n",
    "    Σ_t_t         = np.zeros([T,r,r])\n",
    "    Σ_t_t1[0,:,:] = np.identity(r)\n",
    "    MSE_t_t1      = np.zeros([T,n,n])\n",
    "    K_t           = np.zeros([T,r,n])\n",
    "    ll            = np.zeros([T,1])    \n",
    "    X_t           = endog\n",
    "    \n",
    "    for t in range(0,T-1):\n",
    "        mod=BHL(theta)\n",
    "        #solution of the agent\n",
    "        IKCA, BIG_Ae, BIG_Be, BIG_Ce, Qe = mod.K()\n",
    "        #Σ_t_t1a = np.array([[99999,0,0],\n",
    "        #                    [0,99999,0],\n",
    "        #                    [0,0,1]])\n",
    "        Σ_t_t1a = np.array([[1,0,0],\n",
    "                            [0,1,0],\n",
    "                            [0,0,1]])\n",
    "    \n",
    "          \n",
    "        #Kalman recursion\n",
    "    \n",
    "        X_t_t1[t+1,:,:]   = BIG_Ce@S_t_t1[t,:,:]\n",
    "        MSE_t_t1[t,:,:]   = BIG_Ce@Σ_t_t1[t,:,:]@BIG_Ce.T     \n",
    "        U                 = X_t[:,t+1,:]- X_t_t1[t+1,:,:]\n",
    "        temp            = U.T@LA.pinv(MSE_t_t1[t,:,:])@U  \n",
    "        #log likelihhod    \n",
    "        ll[t]           = -np.log(2*np.pi) -0.5*np.log(LA.det(MSE_t_t1[t,:,:])) -0.5*temp\n",
    "  \n",
    "        K_t[t+1,:,:]      = Σ_t_t1[t,:,:]@BIG_Ce.T@LA.pinv(MSE_t_t1[t,:,:])\n",
    "        S_t_t[t+1,:,:]    = S_t_t1[t,:,:] + K_t[t+1,:,:]@(X_t[:,t+1,:]-X_t_t1[t+1,:,:])\n",
    "        Σ_t_t[t+1,:,:]    = Σ_t_t1[t,:,:] - K_t[t+1,:,:]@BIG_Ce@Σ_t_t1[t,:,:]\n",
    "        S_t_t1[t+1,:,:] = BIG_Ae@S_t_t[t+1,:,:]\n",
    "        Σ_t_t1[t+1,:,:] = BIG_Ae@Σ_t_t[t+1,:,:]@BIG_Ae.T + BIG_Be@Qe@BIG_Be.T\n",
    "        \n",
    "        \n",
    "                             \n",
    "    return -ll.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize                      #import package for scientific computing\n",
    "from datetime import datetime\n",
    "start_time = datetime.now() \n",
    "\n",
    "endog=np.array([a_sim,c_sim])\n",
    "# Minimization of -LogLikelihood, constrained\n",
    "# initial guesses\n",
    "theta_start = np.array([0.5, 0.58**2, .81**2])\n",
    "\n",
    "# Nelder-Mead, BFGS, \n",
    "res = minimize(neg_loglike, theta_start,args=(endog),method=\"Nelder-Mead\",options={'disp': True,'maxiter':10000})\n",
    "end_time = datetime.now()\n",
    "print('Total Time: {}'.format(end_time - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us build our filtered expectations from the estimated parameters\n",
    "\n",
    "r = 6\n",
    "n = 2\n",
    "T            = endog.shape[1]\n",
    "S_t_t1       = np.zeros([T,r,1])\n",
    "S_t_t        = np.zeros([T,r,1])\n",
    "X_t_t1       = np.zeros([T,n,1])\n",
    "Σ_t_t1       = np.zeros([T,r,r])\n",
    "Σ_t_t        = np.zeros([T,r,r])\n",
    "Σ_t_t1[0,:,:]= np.identity(r)\n",
    "MSE_t_t1     = np.zeros([T,n,n])\n",
    "K_t          = np.zeros([T,r,n])\n",
    " \n",
    "X_t          = np.array([a_sim,c_sim])\n",
    "\n",
    "for t in range(0,T-1):\n",
    "    mod=BHL(res.x)\n",
    "    #solution of the agent\n",
    "    IKCA, BIG_Ae, BIG_Be, BIG_Ce, Qe = mod.K()\n",
    "   \n",
    "    #Kalman recursion\n",
    "    \n",
    "    X_t_t1[t+1,:,:]   = BIG_Ce@S_t_t1[t,:,:]\n",
    "    MSE_t_t1[t,:,:] = BIG_Ce@Σ_t_t1[t,:,:]@BIG_Ce.T    \n",
    "    K_t[t+1,:,:]      = Σ_t_t1[t,:,:]@BIG_Ce.T@LA.inv(MSE_t_t1[t,:,:])\n",
    "    S_t_t[t+1,:,:]    = S_t_t1[t,:,:] + K_t[t+1,:,:]@(X_t[:,t+1,:]-X_t_t1[t+1,:,:])\n",
    "    Σ_t_t[t+1,:,:]    = Σ_t_t1[t,:,:] - K_t[t+1,:,:]@BIG_Ce@Σ_t_t1[t,:,:]\n",
    "    S_t_t1[t+1,:,:] = BIG_Ae@S_t_t[t+1,:,:]\n",
    "    Σ_t_t1[t+1,:,:] = BIG_Ae@Σ_t_t[t+1,:,:]@BIG_Ae.T + BIG_Be@Qe@BIG_Be.T\n",
    "                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the filtered expectations\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "plt.subplot(221)\n",
    "plt.plot(S_t_t[:,0,:],label='X_t')\n",
    "plt.plot(S_t_t[:,3,:],label='X_t_t')\n",
    "plt.legend()\n",
    "plt.xlabel('period')\n",
    "plt.grid()\n",
    "plt.subplot(222)\n",
    "plt.plot(S_t_t[:,2,:],label='Z_t')\n",
    "plt.plot(S_t_t[:,5,:],label='Z_t_t')\n",
    "plt.legend()\n",
    "plt.xlabel('period')\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some application we are interested in having an inference os $S_{t}$ based on the full sample, $$\\hat{S}_{t|T}=E\\left[S_{t}|X_{T}\\right]$$ to do obtain it you need to compute the filtered quantities (above) $\\hat{S}_{t|t},\\hat{S}_{t+1|t},\\Sigma_{t|t},\\Sigma_{t+1|t}$ and then $$\\hat{S}_{T-1|T}=\\hat{S}_{T-1|T-1}+J_{T-1}\\left(\\hat{S}_{T|T}-\\hat{S}_{T|T-1}\\right)$$ where $$J_{T-1}=\\Sigma_{T-1|T-1}A'\\Sigma_{T|T-1}^{-1}$$ and proceed backwards. You can also compute the MSE assciated with the smoothed estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smoothing\n",
    "J_t             = np.zeros([T,r,r])\n",
    "S_t_sm          = np.zeros([T,r,1])\n",
    "S_t_sm[T-1,:,:] = S_t_t[T-1,:,:]\n",
    "\n",
    "for t in range(1,T):\n",
    "    J_t[t,:,:] = Σ_t_t[t,:,:]@BIG_Ae.T@LA.pinv(Σ_t_t1[t,:,:])\n",
    "for t in range(1,T):\n",
    "    S_t_sm[T-1-t,:,:] =  S_t_t[T-1-t,:,:] + J_t[T-1,:,:]@(S_t_sm[T-1,:,:] - S_t_t1[T-1,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectations based on the smoothed estimates\n",
    "$$lim_{j\\rightarrow\\infty}E_{t}\\left[x_{t+j}\\right]=\\frac{1}{1-\\rho}\\left(E_{t}\\left[x_{t}\\right]-\\rho E_{t}\\left[x_{t-1}\\right]\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#real time expectations regarding long-run productivity (change in the expectation)\n",
    "S_t_inf = (S_t_sm[:,3,:] - rho*S_t_sm[:,4,:])/(1-rho) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "\n",
    "plt.plot(S_t_sm[:,3,:])\n",
    "plt.plot(S_t_inf)\n",
    "plt.legend(('Smoothed Expectation on x','Real time Expectation base on smoothed x'))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
